{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>■ 복습</b>\n",
    "    머신러닝의 종류 3가지\n",
    "        1. 지도학습\n",
    "        2. 비지도학습\n",
    "        3. 강화학습\n",
    "            강화 + 머신러닝 = DQN\n",
    "\n",
    "    1장. 강화학습이란 무엇인가\n",
    "    \n",
    "    2장. MDP의 구성요소\n",
    "        상태, 행동, 보상, 상태변환 확률, 감가율\n",
    "        특정 환경에서 적응해야할 에이전트가 있으면 그 에이전트가 그 환경에 잘 적응하는지 못하는지를 판단하려면 수학적인 점수가 필요하기 때문에\n",
    "        MDP의 구성요소로 강화학습 시킬 문제를 정의할 필요가 있다.\n",
    "        \n",
    "    3장. Dynamic Programming\n",
    "        특정환경에 에이전트가 적응할 수 있도록 수학적으로 계산해서 환경을 방정식으로 정의하는 것\n",
    "            벨만 방정식\n",
    "                1. 벨만 기대 방정식 : 정책 반복\n",
    "                2. 벨만 최적 방정식 : 가치 반복\n",
    "                \n",
    "    4장. 강화학습 : 경험을 통해서 학습하는 것\n",
    "                1. 살사\n",
    "                2. 큐러닝\n",
    "    \n",
    "    다이나믹 프로그래밍\n",
    "        1. 정책 이터레이션\n",
    "        2. 가치 이터레이션\n",
    "        \n",
    "        다이나믹 프로그래밍 환경에 있는 에이전트는 금수저 집안의 에이전트 부모님이 각 좌표의 가치를 다 계산 -> 가치함수를 통해\n",
    "            가치함수는 각 좌표의 가치를 어떻게 갱신하는가\n",
    "                t+1 시점의 가치를 이용해서 t시점의 가치를 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제1. 가치함수는 상태를 입력받아 앞으로 받을 보상의 합을 출력으로 내놓습니다. 그래서 에이전트는 가치함수를 통해서 어떤 상태에 있는것이 얼마나 좋은지 알 수 있습니다. 그래서 에이전트는 가치함수를 통해서 어떤 상태로 가야할 지 판단 할 수 있습니다. 또 어떤 상태로 가면 좋을지 판단한 후에 그 상태로 가기 위한 행동을 따져 볼 것 입니다. 그런데 만약 어떤 상태에서 어떤 행동이 얼마나 좋은지 알려주는 함수가 있다면 에이전트 입장에서는 더 편하게 행동을 할 수 있을 것 입니다. 그 함수를 뭐라고 합니까 ?\n",
    "    큐함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제2. 큐함수의 수학식은 가치함수와 어떻게 다릅니까 ?\n",
    "    1. 가치함수 수학식\n",
    "![e](http://cfile263.uf.daum.net/image/9986DE3D5F5DA3F42D12A0)\n",
    "\n",
    "    2. 큐함수 수학식 \n",
    "![e2](http://cfile291.uf.daum.net/image/99E61D3C5F5DA42A2C5EB3)\n",
    "\n",
    "    조건문에 행동이 더 들어감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제3. 가치함수와 큐함수에 대한 정책을 고려한 벨만 기대 방정식은 각각 어떻게 됩니까 ?\n",
    "![e](http://cfile294.uf.daum.net/image/99E638345F5DA5372C35BF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제4. 다음의 벨만 기대 방정식을 말로 풀어 설명하면 무엇인가 ?\n",
    "![e](http://cfile258.uf.daum.net/image/999D104C5F5DA6FE2E497F)\n",
    "\n",
    "    특정 정책을 따라갔을 때의 가치함수는 다음 상태의 좌표 가치에 감가율을 곱한 값에 보상을 더한 값의 기대값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제5. 에이전트가 알아야하는 것은 무엇입니까 ?\n",
    "![fig](http://cfile298.uf.daum.net/image/9976BF505F5DA7EF30BFD9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제6. 최적의 큐함수란 무엇인가요 ?\n",
    "![e](http://cfile270.uf.daum.net/image/998C1F435F5DA8DD2DD904)\n",
    "\n",
    "    정책이 최적일 때 그에 따르는 큐함수도 최적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제7. 최적의 정책은 무엇인가요 ? \n",
    "![f](http://cfile284.uf.daum.net/image/99A7904A5F5DA8EE2FE036)\n",
    "<center><b>가능한 행동들 중에서 최고의 큐함수를 가지는 행동을 선택하는 정책</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제8. 일반 정책과 최적의 정책의 차이를 수식으로 나타내시오 !\n",
    "![c](http://cfile267.uf.daum.net/image/99FFE2335F5DBA3504CBB5)\n",
    "\n",
    "    최적의 큐함수 중에서 max를 취하는 것이 최적의 가치함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제9. 위의 식은 가치함수에 대한 최적 방정식 입니다. 그러면 큐함수에 대한 최적 방정식은 어떻게 됩니까?\n",
    "![e](http://cfile266.uf.daum.net/image/9922E24B5F5DBBE306FF96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제10. 지금까지의 내용을 복습 해봅니다.\n",
    "![r](http://cfile260.uf.daum.net/image/99BB804D5F5DBC4D042407)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3장. 다이나믹 프로그래밍과 강화학습의 차이</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제1. 강화학습의 전체 히스토리를 한 눈에 들어오도록 한장으로 표시하면 ?\n",
    "![c](http://cfile244.uf.daum.net/image/994A5F485F6120F0086563)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제2. 다이나믹 프로그래밍이란 무엇입니까 ?\n",
    "    큰 문제안에 작은 문제들이 중첩되어 있는 경우에 전체 큰 문제를 작은 문제로 쪼개서 푸는 것\n",
    "    '다이나믹'이라는 말은 대상이 시간에 따라 변한다, '프로그래밍'이란 말 그대로 '계획' 즉, 여러 프로세서가 다단계로 이뤄지는 것을 말한다.\n",
    "    작은 문제들이 하나의 프로세스가 되는 것이고 이 작은 문제들을 다단계로 풀어가는 프로그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제3. grid world 에서는 다이나믹 프로그래밍이 어떻게 사용되었습니까 ?\n",
    "     하나의 프로세스를 대상으로 문제를 풀어가는 것이 아니라 시간에 따라 다른 프로세스들을 풀어나가는 방식으로 사용\n",
    "![f](http://cfile295.uf.daum.net/image/999ED5475F6123B9086FF5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제4. 책에서 다루는 다이나믹 프로그래밍의 종류 2가지는 무엇입니까 ?\n",
    "![f](http://cfile249.uf.daum.net/image/99745C345F6124FA072D4C)\n",
    "\n",
    "    정책 이터레이션\n",
    "    가치 이터레이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제5. 정책 이터레이션은 grid world 에서의 미로 찾기 게임에서 어떻게 사용되고 있습니까?\n",
    "![f](http://cfile253.uf.daum.net/image/99AC1F375F612B80086D74)\n",
    "\n",
    "    정책 이터레이션은 현재 정책에 대한 참 가치함수를 구하는 정책평가와 평가한 내용을 가지고 정책을 업데이트하는 정책 발전으로 구성\n",
    "        정책을 평가할 때는 벨만 기대 방정식을 이용하며 \n",
    "        정책을 발전할 때는 구한 가치함수를 토대로 최대의 보상을 얻게하는 행동을 선택하는 탐욕정책발전을 이용\n",
    "        \n",
    "![f2](http://cfile260.uf.daum.net/image/999C964A5F612E4B0A57FA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T05:25:18.312148Z",
     "start_time": "2020-09-16T05:25:18.293152Z"
    }
   },
   "outputs": [],
   "source": [
    "def __init__(self, env):\n",
    "        # 환경에 대한 객체 선언\n",
    "        self.env = env\n",
    "        \n",
    "        # 가치함수를 2차원 리스트로 초기화\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "\n",
    "        # 상 하 좌 우 동일한 확률로 정책 초기화\n",
    "        # (5,5) 개수로 각 좌표에서 이동할 수 있는 확률을 저장. 처음에는 전부 0.25의 확률이었다가 improvement 버튼을 누르면 변경\n",
    "        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width for _ in range(env.height)]\n",
    "\n",
    "        # 마침 상태의 설정\n",
    "        self.policy_table[2][2] = []\n",
    "\n",
    "        # 할인율\n",
    "        self.discount_factor = 0.9\n",
    "\n",
    "def policy_evaluation(self):  # 정책평가\n",
    "        # 다음 가치함수 초기화\n",
    "        next_value_table = [[0.00] * self.env.width for _ in range(self.env.height)]\n",
    "\n",
    "        # 모든 상태에 대해서 벨만 기대방정식을 계산\n",
    "        for state in self.env.get_all_states():\n",
    "            # (0,0)에서 (4,4)까지의 모든 좌표(상태)를 하나씩 불러와서 state에 담는다\n",
    "            value = 0.0\n",
    "\n",
    "            # 마침 상태의 가치 함수 = 0\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = value\n",
    "                continue\n",
    "\n",
    "            # 벨만 기대 방정식\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                \n",
    "                # 앞으로 받을 보상을 구한다\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                \n",
    "                # 다음 상태의 가치\n",
    "                next_value = self.get_value(next_state)\n",
    "                \n",
    "                # 가치값 갱신\n",
    "                value += (self.get_policy(state)[action] *\n",
    "                          (reward + self.discount_factor * next_value))\n",
    "            next_value_table[state[0]][state[1]] = value\n",
    "\n",
    "        self.value_table = next_value_table\n",
    "\n",
    "    # 현재 가치 함수에 대해서 탐욕 정책 발전\n",
    "\n",
    "\n",
    "def policy_improvement(self): # 정책 발전\n",
    "    \"\"\"\n",
    "    좌우상하로 [0.25, 0.25, 0.25, 0.25]의 똑같은 확률을 가지고 있는 policy_table에\n",
    "    가치함수 값이 높은 방향으로 더 많은 확률을 부여하게끔 만들어주는 함수\n",
    "    \"\"\"\n",
    "    next_policy = self.policy_table\n",
    "\n",
    "    for state in self.env.get_all_states():        \n",
    "        if state == [2, 2]:\n",
    "            continue\n",
    "        value_list = []\n",
    "\n",
    "        # 반환할 정책 초기화\n",
    "        result = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "        # 모든 행동에 대해서 [보상 + (할인율 * 다음 상태 가치함수)] 계산\n",
    "        for index, action in enumerate(self.env.possible_actions):\n",
    "            next_state = self.env.state_after_action(state, action)\n",
    "            reward = self.env.get_reward(state, action)\n",
    "            next_value = self.get_value(next_state)\n",
    "            value = reward + self.discount_factor * next_value\n",
    "            value_list.append(value)\n",
    "\n",
    "        # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전\n",
    "        max_idx_list = np.argwhere(value_list == np.amax(value_list))\n",
    "        max_idx_list = max_idx_list.flatten().tolist()\n",
    "        prob = 1 / len(max_idx_list)\n",
    "\n",
    "        for idx in max_idx_list:\n",
    "            result[idx] = prob\n",
    "        next_policy[state[0]][state[1]] = result\n",
    "\n",
    "    self.policy_table = next_policy\n",
    "\n",
    "def get_action(self, state):\n",
    "    policy = self.get_policy(state)\n",
    "    policy = np.array(policy)\n",
    "\n",
    "    return np.random.choice(4, 1, p=policy)[0]\n",
    "\n",
    "# 상태에 따른 정책 반환\n",
    "def get_policy(self, state):\n",
    "    return self.policy_table[state[0]][state[1]]\n",
    "\n",
    "# 가치 함수의 값을 반환\n",
    "def get_value(self, state):\n",
    "    return self.value_table[state[0]][state[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제. evaluation버튼을 눌렀을 때 터미널 창에 value_table의 값이 출력되게 하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T05:26:17.473805Z",
     "start_time": "2020-09-16T05:26:17.455809Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(self):  # 정책평가\n",
    "        # 다음 가치함수 초기화\n",
    "        next_value_table = [[0.00] * self.env.width for _ in range(self.env.height)]\n",
    "\n",
    "        # 모든 상태에 대해서 벨만 기대방정식을 계산\n",
    "        for state in self.env.get_all_states():\n",
    "            # (0,0)에서 (4,4)까지의 모든 좌표(상태)를 하나씩 불러와서 state에 담는다\n",
    "            value = 0.0\n",
    "\n",
    "            # 마침 상태의 가치 함수 = 0\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = value\n",
    "                continue\n",
    "\n",
    "            # 벨만 기대 방정식\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                \n",
    "                # 앞으로 받을 보상을 구한다\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                \n",
    "                # 다음 상태의 가치\n",
    "                next_value = self.get_value(next_state)\n",
    "                \n",
    "                # 가치값 갱신\n",
    "                value += (self.get_policy(state)[action] *\n",
    "                          (reward + self.discount_factor * next_value))\n",
    "            next_value_table[state[0]][state[1]] = value\n",
    "            \n",
    "\n",
    "        self.value_table = next_value_table\n",
    "        print(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T05:56:13.570017Z",
     "start_time": "2020-09-16T05:56:13.565015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[[2]]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "value_list = [23,29,30,12]\n",
    "\n",
    "print(np.amax(value_list))\n",
    "\n",
    "rs = np.argwhere(value_list == np.amax(value_list))\n",
    "print(rs)\n",
    "\n",
    "rs2 = rs.flatten().tolist()\n",
    "print(rs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ (오늘의 마지막 문제) 정책 발전 함수의 policy_improvement를 수행할 때 value_list에 [28, 31, 40, 3]이 입력되면 result에 입력되는 확률이 어떻게 되는지 출력하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T06:25:55.428028Z",
     "start_time": "2020-09-16T06:25:55.414033Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "policy_table = [0.25,0.25,0.25,0.25]\n",
    "next_policy = policy_table\n",
    "\n",
    "result = [0.0, 0.0, 0.0, 0.0]\n",
    "value_list = [28, 31, 40, 3]\n",
    "max_idx_list = np.argwhere(value_list == np.amax(value_list))\n",
    "max_idx_list = max_idx_list.flatten()\n",
    "prob = 1 / len(max_idx_list)\n",
    "\n",
    "for idx in max_idx_list:\n",
    "    result[idx] = prob\n",
    "next_policy = result\n",
    "print(next_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
