{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>■ 강화학습 복습</b>\n",
    "    1장. 강화학습이 무엇인지\n",
    "         인공지능 -> 머신러닝 -> 지도학습    :  정답을 보고 공부\n",
    "                               비지도학습  :  데이터의 패턴을 알고리즘이 판단\n",
    "                               강화학습    :  보상\n",
    "         강화학습 : Reinforcement + Machine Learning\n",
    "             정답없이 학습 시킬 수 있다. 보상으로 학습\n",
    "         강화학습 문제를 풀려면 문제를 먼저 수학적으로 정의를 해야한다\n",
    "         강화학습 문제를 수학적으로 정의하기 쉽도록 하기위해서 러시아 수학자 이름을 따서 만든 용어\n",
    "         MDP 구성요소\n",
    "             1. 상태\n",
    "             2. 행동\n",
    "             3. 보상\n",
    "             4. 상태변환 확률\n",
    "             5. 할인율\n",
    "         \n",
    "         강화학습을 쉽게 이해하고 구현할 수 있으려면\n",
    "             1. 간단한 고전게임으로 이해\n",
    "             2. pygame으로 구현\n",
    "             3. 전기효율 / 신약개발/ 반도체 설계 / 스마트 공정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제1. 강화학습은 순차적으로 행동을 계속 결정해야 하는 문제를 푸는 것입니다. 이러한 문제를 수학적으로 표현한것이 무엇입니까 ?\n",
    "    MDP (Markov Decision Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제2. 아래와 같이 격자로 이뤄진 세상(환경) 에서 풀고자 하는 문제는 \"빨간색 네모가 세모를 피해서 파란색 동그라미에 가기\" 입니다. 이 문제를 어떤 식으로 정의해야 컴퓨터가 이해할 수 있을 까요?\n",
    "![fig](http://cfile274.uf.daum.net/image/99BCAF495F5D85631FD14A)\n",
    "\n",
    "    MDP의 구성요소를 정의하고 에이전트가 보상에 따라 확률적으로 행동하도록 하게 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제3. MDP 의 구성 요소중 상태란 무엇인가요 ?\n",
    "![fig](http://cfile286.uf.daum.net/image/99F8E94B5F5D50C80EFE07)\n",
    "\n",
    "    상태는 현재 상황을 나타내는 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제4.  그리드 월드를 수학의 집합으로 나타내면 어떻게 나타낼 수 있습니까?\n",
    "$S = \\{(1,1),(2,1),(3,1), ... ,(5,5)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제5.  어떤 $t$ 에서의 상태 $S_{t}$는 정해져 있지 않습니다. 때에 따라서 $t=1$ 일 때 $S_{t}=(1,3)$ 일 수도 있고 $S_{t}=(4,2)$ 일 수도 있습니다. 이처럼 어떤 집합 안에서 뽑을 때마다 달라질 수 있는 것을 무엇이라 합니까 ?\n",
    "![fig](http://cfile297.uf.daum.net/image/99A0224A5F5D55030F7D36)\n",
    "\n",
    "    확률변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제6.  처음 상태부터 마지막 상태까지의  한 단위를 무엇이라 합니까 ?\n",
    "![fig](http://cfile297.uf.daum.net/image/9980F1365F5D56980EEBC5)\n",
    "\n",
    "    에피소드(Episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제7.  아래의  Grid World 세계에서 에이전트가 선택할 수 있는 행동이 무엇입니까 ? \n",
    "![fig](http://cfile261.uf.daum.net/image/99DC87445F5D59AD0F7D72)\n",
    "\n",
    "    A = {상, 하, 좌, 우}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제8. 시간 t 에 취한 행동을 식으로 표현하면 어떻게 됩니까?\n",
    "$A_{t}=a, a=\\{0,1,2,3\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제9. 만약 시간 $t$ 에서 상태가 $(3, 1)$ 이고 $A_{t} = $ right 라면 다음 시간의 상태는 어떻게 됩니까 ?\n",
    "![fig](http://cfile286.uf.daum.net/image/992E72425F5D5BFD11E633)\n",
    "\n",
    "    (4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제10.  보상이란 무엇입니까 ?\n",
    "    에이전트가 학습할 수 있는 유일한 정보로서 환경이 에이전트에게 주는 정보 (+값 또는 -값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제11.  보상을 수식으로 나타내면 어떻게 나타내야 합니까 ?\n",
    "![fig](http://cfile271.uf.daum.net/image/99776B365F5D5E1F114D50)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제12.  위의 식에서  $t+1$ 이라고 표현한 이유는 무엇입니까?\n",
    "    보상은 t시점이 아니라 t+1 시점의 환경에서 받기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제13.  위의 식에서 기대값 $\\rm{E}$ 로 보상의 식을 구성한 이유가 무엇입니까 ?\n",
    "    같은 상태의 s에서 같은 행동 a를 했다 하더라도 그때마다 보상이 다를 수 있기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제14.  기대값이란 무엇입니까? \n",
    "![fig](http://cfile267.uf.daum.net/image/99A13D335F5D633714FE4F)\n",
    "\n",
    "    확률을 포함한 평균"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제15. 보상에 꼭 있어야하는 정보가 무엇입니까 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제16. 파란색 동그라미 주위에서 파란색 동그라미로 가능 행동을 에이전트가 했다고 가정해 봅시다. 에이전트는 (+1) 의 보상을 다음 타임스텝에서 받을 것 입니다. 하지만 보상 (+1) 을 받았다고 해서 파란색 동그라미에 도착했기 때문에 좋은 보상을 받는 것이 아닙니다. 에이전트는 파란색 동그라미로 가는 좋은 행동을 했기 때문에 보상을 받는 것 입니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제17. 에이전트가 어떤 상테에서 어떤 행동을 취한다면 에이전트의 상태는 변할 것입니다. 아래의 그림처럼 에이전트가 앞으로 나아가는 행동을 하면 s 보다는 s' 상태에 도달 할 것입니다. s'이라는 것은 다음 타임스텝에 에이전트가 갈 수 있는 어떤 특정한 상태를 의미합니다. 하지만 꼭 앞에 있는 상태에 도달하지 못할 수도 있습니다. 옆에서 바람이 불 수 도 있고 갑자기 넘어질 수 도 있습니다. 이처럼 상태의 변화에는 확률적인 요인이 들어갑니다. 이를 수치적으로 표현한 것을 무엇이라고 합니까 ?\n",
    "![fig](http://cfile267.uf.daum.net/image/998E70435F5D66BC141675)\n",
    "\n",
    "    상태변환 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제18. 상태변환 확률을 수식으로 표현하면 어떻게 됩니까 ?\n",
    "![fig](http://cfile269.uf.daum.net/image/997328505F5D677415C9EE)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제19. 강화학습에서 감가율이 필요한 이유가 무엇입니까 ?\n",
    "    같은 보상이면 나중에 받을 수록 가치가 줄어든다는 것을 에이전트에게 알려주기 위해서 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제20. 감가율(할인율) 을 통해서 우리가 얻을 수 있는 것은 무엇입니까 ?\n",
    "![fig](http://cfile250.uf.daum.net/image/99DC87445F5D69F813B7E8)\n",
    "\n",
    "    감가율(할인율)을 통해 보상을 얻는 최적의 경로를 찾을 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제21.Grid world 환경에서 MDP 의 구성요소를 정리하면 어떻게 됩니까 ?\n",
    "![fig](http://cfile293.uf.daum.net/image/99C491495F5D6AD2151A0B)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제22.정책이란 무엇입니까 ?\n",
    "![fig](http://cfile251.uf.daum.net/image/991653395F5D6CCD16CA8E)\n",
    "$$↓$$\n",
    "![fig2](http://cfile295.uf.daum.net/image/998E833D5F5D7FC51C7359)\n",
    "\n",
    "    각 상태에서 어떻게 행동할지에 대한 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제23.정책을 수학식으로 어떻게 나타냅니까 ?\n",
    "![fig](http://cfile263.uf.daum.net/image/99E4DF345F5D80B21AECE4)\n",
    "\n",
    "    에이전트가 강화학습을 통해 학습 해야할 것은 수많은 정책 중에서 최적 정책 입니다. \n",
    "    최적 정책은 각 상태에서 단 하나의 행동만을 선택합니다. \n",
    "    하지만 에이전트가 학습하고 있을 때는 정책이 하나의 행동만을 선택하기보다는 확률적으로 여러 개의 행동을 선택할 수 있어야 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제24. 강화 학습은 현재의 정책보다 더 좋은 정책을 알아내기 위해 학습해 나가는 것입니다. 그러면 현재의 정책보다 더 좋은 정책을 찾아나가기 위해서는 에이전트가 다음 그림처럼 환경에 대해서 행동을 하고 환경은 그 행동에 대한 보상을 하면서 에이전트는 실제로 받은 보상을 토대로 자신의 정책을 바꿔 나갑니다. 그래서 결국 얻게 될 정책은 무슨 정책 입니까 ?\n",
    "![fig](http://cfile259.uf.daum.net/image/99C28C495F5D840C1E6904)\n",
    "\n",
    "    가장 많은 보상을 받게하는 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>2장. 벨만 방정식 이론 문제</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제1. 앞장의 내용을 복습 하세요.\n",
    "![fig](http://cfile260.uf.daum.net/image/99372B425F5D89241F2AAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제2. 아래와 같이 어떤 특정한 상태에 에이전트가 있다고 가정해 보겠습니다. 이 에이전트 입장에서 현재 상태에서 앞으로 받을 보상들을 고려해서 선택해야 좋은 선택을 할 수 있습니다.  하지만 아직 받지 않은 많은 보상들을 어떻게 해야 고려할 수 있을까요?  이를 위해 강화학습에서 나온 개념은 무엇입니까?\n",
    "    가치함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제3. 가치함수를 수학식으로 나타내면 무엇입니까 ?\n",
    "![fig](http://cfile288.uf.daum.net/image/991C313E5F5D92E724F7F6)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제4. 단기적인 보상만을 고려하면 안되고 장기적인 보상을 고려해야 하는 이유가 무엇인가요 ? \n",
    "    대부분의 환경에서의 보상은 장기적인 보상이기 때문\n",
    "![fig](http://cfile259.uf.daum.net/image/99361B425F5D949B24C15D)\n",
    "\n",
    "    즉각적인 보상만 고려해서 어떤 행동이 좋은 행동이었는지 판단한다면 다음과 같은 상황이 발생한다\n",
    "        마지막 행동만 좋은 행동이고 나머지는 아니라고 판단\n",
    "![fig2](http://cfile255.uf.daum.net/image/99DB64445F5D958E231F51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제5. 그러면 장기적인 보상은 어떻게 알 수 있을 까요 ?\n",
    "![fig](http://cfile268.uf.daum.net/image/998F96435F5D9689254471)\n",
    "\n",
    "![fig2](http://cfile264.uf.daum.net/image/99C0E94E5F5D96AC24F139)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제6. 위의 가치함수 식에 Gt 에 반환값을 넣어서 다시 풀어쓰면 어떻게 됩니까 ?\n",
    "![fig](http://cfile291.uf.daum.net/image/99752C505F5D97B229C37A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제7. 에이전트가 앞으로 받을 보상에 대해 생각할 때 정책을 고려하지 않으면 안됩니다. 왜냐하면 상태에서 상태로 넘어갈 때 에이전트는 무조건 행동을 해야하고 각 상태에서 행동을 하는 것이 에이전트의 정책이기 때문입니다. 정책에 따라서 계산하는 가치함수는 당연히 달라질 수 밖에 없습니다. 따라서 가치함수에 정책을 고려한 식으로 다시 가치함수를 표현하면 어떻게 됩니까 ?\n",
    "![fig](http://cfile285.uf.daum.net/image/99194B4D5F5D987225285E)\n",
    "\n",
    "    위의 식에 강화학습에서 상당이 중요한 벨만 기대 방정식 입니다. \n",
    "    벨만 기대 방정식은 현재 상태의 가치함수와 다음 상태의 가치함수 사이의 관계를 말해주는 방정식 입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T05:00:51.799744Z",
     "start_time": "2020-09-15T05:00:51.776740Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from environment import GraphicDisplay, Env\n",
    "\n",
    "\n",
    "class PolicyIteration:\n",
    "    def __init__(self, env):\n",
    "        # 환경에 대한 객체 선언\n",
    "        self.env = env\n",
    "        # 가치함수를 2차원 리스트로 초기화\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        # 상 하 좌 우 동일한 확률로 정책 초기화\n",
    "        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n",
    "                             for _ in range(env.height)]\n",
    "        # 마침 상태의 설정\n",
    "        self.policy_table[2][2] = []\n",
    "        # 할인율\n",
    "        self.discount_factor = 0.9\n",
    "\n",
    "    # 벨만 기대 방정식을 통해 다음 가치함수를 계산하는 정책 평가\n",
    "    def policy_evaluation(self):\n",
    "        # 다음 가치함수 초기화\n",
    "        next_value_table = [[0.00] * self.env.width\n",
    "                            for _ in range(self.env.height)]\n",
    "\n",
    "        # 모든 상태에 대해서 벨만 기대방정식을 계산\n",
    "        for state in self.env.get_all_states():\n",
    "            value = 0.0\n",
    "            # 마침 상태의 가치 함수 = 0\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = value\n",
    "                continue\n",
    "\n",
    "            # 벨만 기대 방정식\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value += (self.get_policy(state)[action] *\n",
    "                          (reward + self.discount_factor * next_value))\n",
    "\n",
    "            next_value_table[state[0]][state[1]] = value\n",
    "\n",
    "        self.value_table = next_value_table\n",
    "\n",
    "    # 현재 가치 함수에 대해서 탐욕 정책 발전\n",
    "    def policy_improvement(self):\n",
    "        next_policy = self.policy_table\n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                continue\n",
    "\n",
    "            value_list = []\n",
    "            # 반환할 정책 초기화\n",
    "            result = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "            # 모든 행동에 대해서 [보상 + (할인율 * 다음 상태 가치함수)] 계산\n",
    "            for index, action in enumerate(self.env.possible_actions):\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value = reward + self.discount_factor * next_value\n",
    "                value_list.append(value)\n",
    "\n",
    "            # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전\n",
    "            max_idx_list = np.argwhere(value_list == np.amax(value_list))\n",
    "            max_idx_list = max_idx_list.flatten().tolist()\n",
    "            prob = 1 / len(max_idx_list)\n",
    "\n",
    "            for idx in max_idx_list:\n",
    "                result[idx] = prob\n",
    "\n",
    "            next_policy[state[0]][state[1]] = result\n",
    "\n",
    "        self.policy_table = next_policy\n",
    "\n",
    "    # 특정 상태에서 정책에 따라 무작위로 행동을 반환\n",
    "    def get_action(self, state):\n",
    "        policy = self.get_policy(state)\n",
    "        policy = np.array(policy)\n",
    "        return np.random.choice(4, 1, p=policy)[0]\n",
    "\n",
    "    # 상태에 따른 정책 반환\n",
    "    def get_policy(self, state):\n",
    "        return self.policy_table[state[0]][state[1]]\n",
    "\n",
    "    # 가치 함수의 값을 반환\n",
    "    def get_value(self, state):\n",
    "        return self.value_table[state[0]][state[1]]\n",
    "\n",
    "\n",
    "env = Env()\n",
    "policy_iteration = PolicyIteration(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩문제1. PolicyIteration 클래스의 value_table 변수의 결과를 출력하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T05:00:52.787617Z",
     "start_time": "2020-09-15T05:00:52.778615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print( policy_iteration.value_table )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩문제2. PolicyIteration 클래스의 policy_table 변수의 결과를 출력하시오 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T05:02:35.027505Z",
     "start_time": "2020-09-15T05:02:35.017502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], [[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], [[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], [[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], [[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]]]\n"
     ]
    }
   ],
   "source": [
    "print(policy_iteration.policy_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig](http://cfile290.uf.daum.net/image/99547C385F5F780C2C0256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩문제3.  imporve 와 eval‎uate 를 눌렀을 때 각 자리의 가치의 변경된 결과가 계속 출력되게하시오 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T05:31:21.253540Z",
     "start_time": "2020-09-15T05:31:21.239544Z"
    }
   },
   "outputs": [],
   "source": [
    "policy_iteration.policy_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩문제4. 아래와 같이 eval‎udate 를 한번 눌렀을때 나오는 각 좌표의 가치값이 터미널 창에서 출력되게 코드로 구현하시오 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_iteration.policy_evaluation()\n",
    "policy_iteration.policy_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩문제5. 아래의 가치함수 수학식을 구현한 아래의 코드에서 출력되고 있는 get_policy(state)[action] 의 값을 출력하시오~\n",
    "![fig](http://cfile296.uf.daum.net/image/9961D84B5F5F259932D25E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T05:40:38.219525Z",
     "start_time": "2020-09-15T05:40:38.202521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25, 0.25, 0.25, 0.25]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_iteration.get_policy([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
