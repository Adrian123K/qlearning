{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>■ 복습</b>\n",
    "    머신러닝의 종류 3가지? \n",
    "    1. 지도학습 : 데이터의 정답을 가지고 학습\n",
    "    2. 비지도 학습 : 데이터의 정답은 없지만 데이터의 패턴을 보고 학습 3. 강화학습 : 보상을 통해서 학습\n",
    "    \n",
    "    1장\n",
    "    2장\n",
    "    3장 : 학습 x \n",
    "    4장\n",
    "    5장\n",
    "    6장 : 학습 o\n",
    "\n",
    "    강화학습 : Reinforcement + Machine Learning\n",
    "        더 많은 보상을 받기 위해 학습을 강화시킨다. --> 자동화\n",
    "        본인 포트폴리오(신경망) + 강화학습 포트폴리오"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제6. 가치 이터레이션은 grid world 에서의 미로 찾기 게임에서 어떻게 사용되고 있습니까?\n",
    "    가치 이터레이션은 최적 정책을 가정하고 벨만 최적 방정식을 이용해 순차적 행동 결정 문제에 접근\n",
    "    정책 이터레이션에서와 달리 정책이 직접적으로 주어지지 않으며 행동의 선택은 가치함수를 통해 이뤄진다\n",
    "        정책 이터레이션 : 평가 + 개선\n",
    "        가치 이터레이션 : 계산\n",
    "![f](http://cfile288.uf.daum.net/image/999A6C475F6132610B7D87)\n",
    "![f2](http://cfile260.uf.daum.net/image/99340E445F61320009F992)\n",
    "![f3](http://cfile298.uf.daum.net/image/99992B4A5F61321F0B742C)\n",
    "![f4](http://cfile258.uf.daum.net/image/995F4B4F5F624E86018D57)\n",
    "![f5](http://cfile252.uf.daum.net/image/993649485F624E9F01BDF9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    정책 이터레이션은 정책 평가와 정책개선을 반복해서 최적의 정책을 찾고\n",
    "    가치 이터레이션은 최적의 가치 함수를 구하는 계산만으로 최적의 정책을 찾는다\n",
    "    \n",
    "    가치 이터레이션에서 각 자리의 가치 함수값만으로는 move 할 수 없고 행동해야 할 방향을 알려주는 정책(policy)을 알아야 move 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T01:46:22.565507Z",
     "start_time": "2020-09-17T01:46:22.553505Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 0.0, 0.0]\n",
      "0.45\n",
      "[[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.45, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "policy_table = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "#value_list = [28,31,40,3]\n",
    "value_list = [0.32, 0.44, 0.21, 0.22]  # 상하좌우\n",
    "next_value_table = [[0.0] * 5 for _ in range(5)]\n",
    "max_idx_list = np.argwhere(value_list == np.amax(value_list))\n",
    "max_idx_list = max_idx_list.flatten().tolist()\n",
    "\n",
    "prob = 1 / len(max_idx_list)\n",
    "\n",
    "for idx in max_idx_list:\n",
    "    policy_table[idx] = prob\n",
    "\n",
    "print(policy_table)\n",
    "\n",
    "value = 0.0\n",
    "next_value = [0, 0.5, 1, 0] # 이동했을 때 상하좌우의 가치\n",
    "reward = [0, 0, 0, 1] # 이동했을 떄의 상하좌우 보상\n",
    "\n",
    "for action in [0, 1, 2, 3]:\n",
    "    #value += (self.get_policy(state)[action] * (reward + self.discount_factor * next_value))\n",
    "    value += (policy_table[action] * (reward[action] + 0.9 * next_value[action]))\n",
    "\n",
    "print(value)\n",
    "\n",
    "next_value_table[1][2] = value\n",
    "print(next_value_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    위의 코드는 특정 좌표의 가치값을 정책 이터레이션으로 갱신해 나가는 코드인데 25개의 좌표중 일단 (1,2) 자리만 갱신해 보아본 코드이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제1. 위의 코드를 그림으로 설명하세요.  \n",
    "    아래의 숫자값은 각 좌표의 가치이고 가운데 ? 의 다음번 가치를 출력해야하는 상황임. \n",
    "    정책 이터레이션임으로 벨만 기대방적식을 이용해서 다음번 가치를 출력하고 갱신함.\n",
    "\n",
    "             0.32\n",
    "     0.21      ?     0.22 \n",
    "             0.44 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제2. 그럼 아래의 상황일 때 다음번 ? 의 좌표의 가치함수는 어떻게 되는가?\n",
    "     0.20 0.53 0.32 \n",
    "     0.44   ?  0.88 \n",
    "     0.39 0.21 0.45 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T02:16:33.899135Z",
     "start_time": "2020-09-17T02:16:33.893134Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "policy_table = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "#value_list = [28,31,40,3]\n",
    "value_list = [0.53, 0.21, 0.44, 0.88]  # 상하좌우\n",
    "next_value_table = [[0.0] * 5 for _ in range(5)]\n",
    "max_idx_list = np.argwhere(value_list == np.amax(value_list))\n",
    "max_idx_list = max_idx_list.flatten().tolist()\n",
    "\n",
    "prob = 1 / len(max_idx_list)\n",
    "\n",
    "for idx in max_idx_list:\n",
    "    policy_table[idx] = prob\n",
    "\n",
    "value = 0.0\n",
    "next_value = [0, 0.5, 1, 0] # 이동했을 때 상하좌우의 가치\n",
    "reward = [0, 0, 0, 1] # 이동했을 떄의 상하좌우 보상\n",
    "\n",
    "for action in [0, 1, 2, 3]:\n",
    "    #value += (self.get_policy(state)[action] * (reward + self.discount_factor * next_value))\n",
    "    value += (policy_table[action] * (reward[action] + 0.9 * next_value[action]))\n",
    "\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가치 이터레이션\n",
    "![f](http://cfile290.uf.daum.net/image/9934F1495F62573B030D50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제3. 위의 정책 이터레이션 코드를 가치 이터레이션 코드로 변경하시오 ~ 아래의 가치 이터레이션 코드를 참고하세요.\n",
    "          0.32\n",
    "    0.21    ?   0.22 \n",
    "          0.44 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "from environment import GraphicDisplay, Env\n",
    "\n",
    "class ValueIteration:\n",
    "    def __init__(self, env):\n",
    "        # 환경에 대한 객체 선언\n",
    "        self.env = env\n",
    "\n",
    "        # 가치 함수를 2차원 리스트로 초기화\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "\n",
    "        # 할인율\n",
    "        self.discount_factor = 0.9\n",
    "\n",
    "    # 벨만 최적 방정식을 통해 다음 가치 함수 계산\n",
    "    def value_iteration(self):\n",
    "        # 다음 가치함수 초기화\n",
    "        next_value_table = [[0.0] * self.env.width\n",
    "                            for _ in range(self.env.height)]\n",
    "\n",
    "        # 모든 상태에 대해서 벨만 최적방정식을 계산\n",
    "        for state in self.env.get_all_states():\n",
    "            # 마침 상태의 가치 함수 = 0\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = 0.0\n",
    "                continue\n",
    "            # 벨만 최적 방정식\n",
    "            value_list = []\n",
    "\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value_list.append((reward + self.discount_factor * next_value))\n",
    "\n",
    "            # 최댓값을 다음 가치 함수로 대입\n",
    "            next_value_table[state[0]][state[1]] = max(value_list)\n",
    "\n",
    "        self.value_table = next_value_table\n",
    "\n",
    "    # 현재 가치 함수로부터 행동을 반환\n",
    "    def get_action(self, state):\n",
    "        if state == [2, 2]:\n",
    "            return []\n",
    "\n",
    "        # 모든 행동에 대해 큐함수 (보상 + (감가율 * 다음 상태 가치함수))를 계산\n",
    "        value_list = []\n",
    "        for action in self.env.possible_actions:\n",
    "            next_state = self.env.state_after_action(state, action)\n",
    "            reward = self.env.get_reward(state, action)\n",
    "            next_value = self.get_value(next_state)\n",
    "            value = (reward + self.discount_factor * next_value)\n",
    "            value_list.append(value)\n",
    "\n",
    "        # 최대 큐 함수를 가진 행동(복수일 경우 여러 개)을 반환\n",
    "        max_idx_list = np.argwhere(value_list == np.amax(value_list))\n",
    "        action_list = max_idx_list.flatten().tolist()\n",
    "\n",
    "        return action_list\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return self.value_table[state[0]][state[1]]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    value_iteration = ValueIteration(env)\n",
    "    grid_world = GraphicDisplay(value_iteration)\n",
    "    grid_world.mainloop()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T02:33:19.672822Z",
     "start_time": "2020-09-17T02:33:19.664819Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "policy_table = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "#value_list = [28,31,40,3]\n",
    "value_list = [0.32, 0.44, 0.21, 0.22]  # 상하좌우\n",
    "next_value_table = [[0.0] * 5 for _ in range(5)]\n",
    "max_idx_list = np.argwhere(value_list == np.amax(value_list))\n",
    "max_idx_list = max_idx_list.flatten().tolist()\n",
    "\n",
    "prob = 1 / len(max_idx_list)\n",
    "\n",
    "for idx in max_idx_list:\n",
    "    policy_table[idx] = prob\n",
    "\n",
    "value = 0.0\n",
    "next_value = [0, 0.5, 1, 0] # 이동했을 때 상하좌우의 가치\n",
    "reward = [0, 0, 0, 1] # 이동했을 떄의 상하좌우 보상\n",
    "\n",
    "value_list = []\n",
    "for action in [0, 1, 2, 3]:\n",
    "    #value += (self.get_policy(state)[action] * (reward + self.discount_factor * next_value))\n",
    "    value_list.append((reward[action] + 0.9 * next_value[action]))\n",
    "next_value_table[1][2] = max(value_list)\n",
    "\n",
    "print(max(value_list))\n",
    "print(next_value_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    정책 이터레이션으로 각 좌표의 가치함수값을 갱신하려면 상하좌우에 대한 확률값이 필요\n",
    "    가치 이터레이션에서는 확률은 필요없고 상하좌우에 대한 보상+0.9*(다음상태의 가치함수 값)에 대한 최댓값을 취함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제7. 다이나믹 프로그래밍의 한계가 무엇입니까 ?\n",
    "    1. 그리드월드는 5x5에 불과한 작은 크기의 문제지만 규모가 늘어난다면 계산만으로 풀어내기에는 한계가 존재\n",
    "        다이나믹 프로그래밍의 계산 복잡도는 상태 크기의 3제곱에 비례\n",
    "    2. 그리드월드는 2차원이지만 차원이 더 늘어난다면 상태의 수가 지수적으로 증가\n",
    "        차원의 저주\n",
    "    3. 환경에 대한 완벽한 정보가 필요    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제8. 그러면 이런 다이나믹 프로그램의 한계를 극복하려면 어떻게 해야 합니까 ?\n",
    "    환경을 완벽하게 다 이해하고 다 계산하는 것은 불가능하니 환경을 모르지만 환경과의 상호작용을 통해 경험을 바탕으로 학습하는 방법이 필요\n",
    "        강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제9. 환경을 안다는 것은 환경을 모델링 할 수 있다는 것입니다. 모델링이란 어떤 수학적 모델로 시스템에 어떤 입력이 들어왔을때 어떤 출력을 내는지에 대한 방정식 입니다. 세상의 수많은 환경과 현상을 정확히 모델링 할 수는 없습니다. 그래서 모델링 없이 환경에 적응해 나가는 방법이 연구 되었는데 그게 무엇입니까 ?\n",
    "    모델링 없이 학습하는 학습 개념이 들어간 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    다이나믹 프로그래밍은 우리가 다 가치를 계산해놓고 에이전트에게 좋은길로 움직여라고 move 했을뿐이다.\n",
    "    그런데 다이나믹 프로그래밍으로는 그리드 월드 정도는 해결할 수 있지만 바둑같은 문제를 풀 수 는 없다. \n",
    "    그래서 나온게 \n",
    "    \n",
    "\n",
    "![f](http://cfile260.uf.daum.net/image/99E160435F61382C0A6CB6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
