{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> ■ 복습</b>\n",
    "    1장. 강화학습 개념\n",
    "        강화 + 머신러닝 ( 데이터를 통해서 스스로 학습하는 알고리즘 ) : 신경망\n",
    "            보상을 통해서 인공신경망을 학습시키는 것\n",
    "    \n",
    "    2장. 순차적 문제들의 세상(환경)을 해결하려면 세상에 대한 수학적 정의가 필요한데 수학적 도구 2가지\n",
    "        1. MDP\n",
    "        2. 벨만 방정식\n",
    "    \n",
    "    3장. 다이나믹 프로그래밍\n",
    "        1. 정책 이터레이션 (벨만 기대방정식)\n",
    "        2. 가치 이터레이션 (벨만 최적방정식)\n",
    "        \n",
    "        에이전트에게 만들어준 환경을 에이전트가 잘 살아갈 수 있도록 계산해주는 것\n",
    "        \n",
    "        차원이 복잡해지고 차원수가 늘어나면 계산이 오래걸리므로 학습이 필요\n",
    "        \n",
    "    4장. 강화학습\n",
    "        1. SARSA : 시간차 학습방법으로 바로바로 학습하는 강화학습 방법\n",
    "            On policy : 벨만 기대 방정식\n",
    "        2. QLearning : SARSA처럼 바로바로 학습을 하는데 행동과 학습이 분리된 방법\n",
    "            Off policy : 벨만 최적 방정식\n",
    "        \n",
    "        장애물이 움직이는 환경이면 SARSA와 QLearning으로는 해결할 수 없다. 그래서 인공 신경망이 필요\n",
    "        \n",
    "    5장. DeepSARSA (SARSA + ANN)\n",
    "        Q_TABLE을 인공 신경망으로 근사하는 방법을 사용해서 에이전트가 환경에 스스로 적응해 나가도록 구현\n",
    "        \n",
    "        DeepSARSA 코드 구현\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제3. 딥살사 클래스 함수중 train_model 함수의 tf.GradientTape() 가 무엇인가요? \n",
    "```python\n",
    "def train_model(self, state, action, reward, next_state, next_action, done):\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # 학습 파라메터\n",
    "    model_params = self.model.trainable_variables\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model_params)\n",
    "        predict = self.model(state)[0]\n",
    "        one_hot_action = tf.one_hot([action], self.action_size)\n",
    "        predict = tf.reduce_sum(one_hot_action * predict, axis=1)\n",
    "\n",
    "        # done = True 일 경우 에피소드가 끝나서 다음 상태가 없음\n",
    "\n",
    "        next_q = self.model(next_state)[0][next_action]\n",
    "        target = reward + (1 - done) * self.discount_factor * next_q\n",
    "\n",
    "        # MSE 오류 함수 계산\n",
    "        loss = tf.reduce_mean(tf.square(target - predict))\n",
    "\n",
    "    # 오류함수를 줄이는 방향으로 모델 업데이트\n",
    "    grads = tape.gradient(loss, model_params)\n",
    "\n",
    "    self.optimizer.apply_gradients(zip(grads, model_params))\n",
    "```\n",
    "\n",
    "    Tensorflow 2의 tape 기능을 이용하면 신경망 학습 중에 발생하는 기울기 값을 확인할 수 있다.\n",
    "    기울기 값을 뽑을 수 있게 되면 그래프로 시각화 할 수 도 있으며 기울기가 점점 작아지는 것을 확인함으로써\n",
    "    학습이 잘되고 있는지 Local Minima에 빠지지 않았는지 등을 확인할 수 있어 유용하다.\n",
    "    \n",
    "    날코딩했던 때는 기울기를 직접 뽑을 수 있었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    x_batch = x  # 어차피 1장이기때문에 x 만 넣는다.\n",
    "    t_batch = t[:1]\n",
    "    grad = net.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'W2', 'b1', 'b2'):\n",
    "        net.params[key] -= 0.01 * grad[key]\n",
    "    #acc = net.accuracy(x_batch, t_batch)\n",
    "    y = net.predict(x_batch)\n",
    "    y_hat = np.argmax(y, axis=1)\n",
    "    print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    케라스로 넘어오면서 아래와 같이 기울기는 케라스 내장함수에 돌고 model.fit 만 하면 되었다.\n",
    "    그래서 기울기를 확인할 수 있는 길이 없어졌다. \n",
    "\n",
    "```python\n",
    "import  tensorflow  as   tf\n",
    "tf.random.set_seed(777)  # 시드를 설정한다.\n",
    "\n",
    "import  numpy  as  np\n",
    "from  tensorflow.keras.models  import  Sequential   # 신경망 모델 구성\n",
    "from  tensorflow.keras.layers  import  Dense  # 완전 연결계층 \n",
    "from  tensorflow.keras.optimizers  import   SGD  # 경사감소법 \n",
    "from  tensorflow.keras.losses   import   mse    #  오차함수 \n",
    "\n",
    "# 데이터 준비\n",
    "x = np.array( [ [0, 0], [1, 0], [0, 1], [1, 1] ] )\n",
    "y = np.array( [ [0], [0], [0], [1] ] )\n",
    "\n",
    "#모델 구성하기 \n",
    "model = Sequential()\n",
    "\n",
    "#단층 퍼셉트론 구현하기\n",
    "model.add( Dense( 1, input_shape =( 2,  ), activation ='linear')  ) \n",
    "\n",
    "# 모델 준비하기\n",
    "model.compile( optimizer= SGD(), \n",
    "                     loss= mse, \n",
    "                     metrics = ['acc'] ) # list 형태로 평가지표를 전달한다.  \n",
    "\n",
    "# 학습 시키기 \n",
    "model.fit(x, y, epochs = 500) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "print(x)\n",
    "\n",
    "'''\n",
    "tf.Tensor(\n",
    "[[1. 1.]\n",
    " [1. 1.]], shape=(2, 2), dtype=float32)\n",
    "'''\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x) # tape에 기록\n",
    "    y = tf.reduce_sum(x)\n",
    "print(y)\n",
    "\n",
    "# tf.Tensor(4.0, shape=(), dtype=float32)\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x) # y=4x\n",
    "    z = tf.multiply(y, y) # z = y^2\n",
    "\n",
    "print(z)\n",
    "\n",
    "# tf.Tensor(16.0, shape=(), dtype=float32)\n",
    "\n",
    "dz_dx = t.gradient(z, x) # z를 x에 대해 미분\n",
    "print(dz_dx)\n",
    "'''\n",
    "tf.Tensor(\n",
    "[[8. 8.]\n",
    " [8. 8.]], shape=(2, 2), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    위의 설명 사이트의 코드를  신경망으로 코렙에서 수행할 수 있도록 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train.min(), x_train.max()\n",
    "\n",
    "# 차원을 1 늘려줍니다\n",
    "x_train = tf.expand_dims(x_train, -1)\n",
    "x_test = tf.expand_dims(x_test, -1)\n",
    "\n",
    "x_train.shape, x_test.shape\n",
    "\n",
    "\n",
    "# Dataset을 만듭니다\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "\n",
    "#STEP 1. Model을 정의합니다.\n",
    "\n",
    "# layer 정의\n",
    "input_ = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "x = tf.keras.layers.Conv2D(32, 3, activation='relu')(input_)\n",
    "x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "output_ = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "\n",
    "\n",
    "# model을 정의\n",
    "model = tf.keras.models.Model(input_, output_)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "#STEP 2. Loss Function을 정의합니다.\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "#STEP 3. Optimizer를 정의합니다.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# STEP 4. Metric을 정의합니다.\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "test_loss = tf.keras.metrics.Mean()\n",
    "test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "# STEP 5. Train/Test step 함수를 정의합니다.\n",
    "\n",
    "'''\n",
    "GradientTape (그라디언트 테이프)\n",
    "텐서플로는 자동 미분(주어진 입력 변수에 대한 연산의 그래디언트(gradient)를 계산하는 것) 을 위한 tf.GradientTape API를 제공합니다.\n",
    "\n",
    "tf.GradientTape는 컨텍스트(context) 안에서 실행된 모든 연산을 테이프(tape)에 \"기록\"합니다.\n",
    "\n",
    "그 다음 텐서플로는 후진 방식 자동 미분(reverse mode differentiation)을 사용해 테이프에 \"기록된\" 연산의 그래디언트를 계산합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    # 미분을 위한 GradientTape을 적용합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. 예측 (prediction)\n",
    "        predictions = model(images)\n",
    "        # 2. Loss 계산\n",
    "        loss = loss_function(labels, predictions)\n",
    "    \n",
    "    # 3. 그라디언트(gradients) 계산\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # 4. 오차역전파(Backpropagation) - weight 업데이트\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # loss와 accuracy를 업데이트 합니다.\n",
    "    train_loss(loss)\n",
    "    train_acc(labels, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # 1. 예측 (prediction)\n",
    "    predictions = model(images)\n",
    "    # 2. Loss 계산\n",
    "    loss = loss_function(labels, predictions)\n",
    "    \n",
    "    # Test셋에 대해서는 gradient를 계산 및 backpropagation 하지 않습니다.\n",
    "    \n",
    "    # loss와 accuracy를 업데이트 합니다.\n",
    "    test_loss(loss)\n",
    "    test_acc(labels, predictions)\n",
    "    \n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    template = '에포크: {}, 손실: {:.5f}, 정확도: {:.2f}%, 테스트 손실: {:.5f}, 테스트 정확도: {:.2f}%'\n",
    "    print (template.format(epoch+1,\n",
    "                           train_loss.result(),\n",
    "                           train_acc.result()*100,\n",
    "                           test_loss.result(),\n",
    "                           test_acc.result()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제. 아래의 3x3의 1로 채워진 x행렬을 만들고 아래의 z함수를 x에 대해서 미분한 미분계수(기울기)를 출력하시오\n",
    "    y = 9x\n",
    "    z = y^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones(3, 3)\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x) \n",
    "    z = tf.multiply(y, y, y) \n",
    "\n",
    "dz_dx = t.gradient(z, x) \n",
    "print(dz_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  문제4. train_model 함수 안에 self.model.trainable_variables 안의 내용이 무엇인가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(self, state, action, reward, next_state, next_action, done):\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # 학습 파라메터\n",
    "    model_params = self.model.trainable_variables\n",
    "    print(model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제5. 상태를 인공신경망에 넣었을때 출력되는 예측값은 무엇인가요 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(model_params)\n",
    "    predict = self.model(state)[0]\n",
    "    print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제6. one_hot_action 에서 출력되는 결과는 무엇인가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(model_params)\n",
    "    predict = self.model(state)[0]\n",
    "\n",
    "    one_hot_action = tf.one_hot([action], self.action_size)\n",
    "    print(one_hot_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제7. 최종 predict 에서 출력되는 결과는 무엇인가요 ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(self, state, action, reward, next_state, next_action, done):\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # 학습 파라메터\n",
    "    model_params = self.model.trainable_variables\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model_params)\n",
    "        predict = self.model(state)[0]\n",
    "\n",
    "        one_hot_action = tf.one_hot([action], self.action_size)\n",
    "\n",
    "        predict = tf.reduce_sum(one_hot_action * predict, axis=1)\n",
    "        print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제8. 출력되는 결과를 확인하세요 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
