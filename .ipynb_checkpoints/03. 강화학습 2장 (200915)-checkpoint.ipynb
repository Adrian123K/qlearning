{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>■ 강화학습 복습</b>\n",
    "    1장. 강화학습이 무엇인지\n",
    "         인공지능 -> 머신러닝 -> 지도학습    :  정답을 보고 공부\n",
    "                               비지도학습  :  데이터의 패턴을 알고리즘이 판단\n",
    "                               강화학습    :  보상\n",
    "         강화학습 : Reinforcement + Machine Learning\n",
    "             정답없이 학습 시킬 수 있다. 보상으로 학습\n",
    "         강화학습 문제를 풀려면 문제를 먼저 수학적으로 정의를 해야한다\n",
    "         강화학습 문제를 수학적으로 정의하기 쉽도록 하기위해서 러시아 수학자 이름을 따서 만든 용어\n",
    "         MDP 구성요소\n",
    "             1. 상태\n",
    "             2. 행동\n",
    "             3. 보상\n",
    "             4. 상태변환 확률\n",
    "             5. 할인율\n",
    "         \n",
    "         강화학습을 쉽게 이해하고 구현할 수 있으려면\n",
    "             1. 간단한 고전게임으로 이해\n",
    "             2. pygame으로 구현\n",
    "             3. 전기효율 / 신약개발/ 반도체 설계 / 스마트 공정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제1. 강화학습은 순차적으로 행동을 계속 결정해야 하는 문제를 푸는 것입니다. 이러한 문제를 수학적으로 표현한것이 무엇입니까 ?\n",
    "    MDP (Markov Decision Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제2. 아래와 같이 격자로 이뤄진 세상(환경) 에서 풀고자 하는 문제는 \"빨간색 네모가 세모를 피해서 파란색 동그라미에 가기\" 입니다. 이 문제를 어떤 식으로 정의해야 컴퓨터가 이해할 수 있을 까요?\n",
    "![fig](http://cfile274.uf.daum.net/image/99BCAF495F5D85631FD14A)\n",
    "\n",
    "    MDP의 구성요소를 정의하고 에이전트가 보상에 따라 확률적으로 행동하도록 하게 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제3. MDP 의 구성 요소중 상태란 무엇인가요 ?\n",
    "![fig](http://cfile286.uf.daum.net/image/99F8E94B5F5D50C80EFE07)\n",
    "\n",
    "    상태는 현재 상황을 나타내는 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제4.  그리드 월드를 수학의 집합으로 나타내면 어떻게 나타낼 수 있습니까?\n",
    "$S = \\{(1,1),(2,1),(3,1), ... ,(5,5)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제5.  어떤 $t$ 에서의 상태 $S_{t}$는 정해져 있지 않습니다. 때에 따라서 $t=1$ 일 때 $S_{t}=(1,3)$ 일 수도 있고 $S_{t}=(4,2)$ 일 수도 있습니다. 이처럼 어떤 집합 안에서 뽑을 때마다 달라질 수 있는 것을 무엇이라 합니까 ?\n",
    "![fig](http://cfile297.uf.daum.net/image/99A0224A5F5D55030F7D36)\n",
    "\n",
    "    확률변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제6.  처음 상태부터 마지막 상태까지의  한 단위를 무엇이라 합니까 ?\n",
    "![fig](http://cfile297.uf.daum.net/image/9980F1365F5D56980EEBC5)\n",
    "\n",
    "    에피소드(Episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제7.  아래의  Grid World 세계에서 에이전트가 선택할 수 있는 행동이 무엇입니까 ? \n",
    "![fig](http://cfile261.uf.daum.net/image/99DC87445F5D59AD0F7D72)\n",
    "\n",
    "    A = {상, 하, 좌, 우}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제8. 시간 t 에 취한 행동을 식으로 표현하면 어떻게 됩니까?\n",
    "$A_{t}=a, a=\\{0,1,2,3\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제9. 만약 시간 $t$ 에서 상태가 $(3, 1)$ 이고 $A_{t} = $ right 라면 다음 시간의 상태는 어떻게 됩니까 ?\n",
    "![fig](http://cfile286.uf.daum.net/image/992E72425F5D5BFD11E633)\n",
    "\n",
    "    (4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제10.  보상이란 무엇입니까 ?\n",
    "    에이전트가 학습할 수 있는 유일한 정보로서 환경이 에이전트에게 주는 정보 (+값 또는 -값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제11.  보상을 수식으로 나타내면 어떻게 나타내야 합니까 ?\n",
    "![fig](http://cfile271.uf.daum.net/image/99776B365F5D5E1F114D50)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제12.  위의 식에서  $t+1$ 이라고 표현한 이유는 무엇입니까?\n",
    "    보상은 t시점이 아니라 t+1 시점의 환경에서 받기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제13.  위의 식에서 기대값 $\\rm{E}$ 로 보상의 식을 구성한 이유가 무엇입니까 ?\n",
    "    같은 상태의 s에서 같은 행동 a를 했다 하더라도 그때마다 보상이 다를 수 있기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제14.  기대값이란 무엇입니까? \n",
    "![fig](http://cfile267.uf.daum.net/image/99A13D335F5D633714FE4F)\n",
    "\n",
    "    확률을 포함한 평균"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제15. 보상에 꼭 있어야하는 정보가 무엇입니까 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제16. 파란색 동그라미 주위에서 파란색 동그라미로 가능 행동을 에이전트가 했다고 가정해 봅시다. 에이전트는 (+1) 의 보상을 다음 타임스텝에서 받을 것 입니다. 하지만 보상 (+1) 을 받았다고 해서 파란색 동그라미에 도착했기 때문에 좋은 보상을 받는 것이 아닙니다. 에이전트는 파란색 동그라미로 가는 좋은 행동을 했기 때문에 보상을 받는 것 입니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제17. 에이전트가 어떤 상테에서 어떤 행동을 취한다면 에이전트의 상태는 변할 것입니다. 아래의 그림처럼 에이전트가 앞으로 나아가는 행동을 하면 s 보다는 s' 상태에 도달 할 것입니다. s'이라는 것은 다음 타임스텝에 에이전트가 갈 수 있는 어떤 특정한 상태를 의미합니다. 하지만 꼭 앞에 있는 상태에 도달하지 못할 수도 있습니다. 옆에서 바람이 불 수 도 있고 갑자기 넘어질 수 도 있습니다. 이처럼 상태의 변화에는 확률적인 요인이 들어갑니다. 이를 수치적으로 표현한 것을 무엇이라고 합니까 ?\n",
    "![fig](http://cfile267.uf.daum.net/image/998E70435F5D66BC141675)\n",
    "\n",
    "    상태변환 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제18. 상태변환 확률을 수식으로 표현하면 어떻게 됩니까 ?\n",
    "![fig](http://cfile269.uf.daum.net/image/997328505F5D677415C9EE)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제19. 강화학습에서 감가율이 필요한 이유가 무엇입니까 ?\n",
    "    같은 보상이면 나중에 받을 수록 가치가 줄어든다는 것을 에이전트에게 알려주기 위해서 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제20. 감가율(할인율) 을 통해서 우리가 얻을 수 있는 것은 무엇입니까 ?\n",
    "![fig](http://cfile250.uf.daum.net/image/99DC87445F5D69F813B7E8)\n",
    "\n",
    "    감가율(할인율)을 통해 보상을 얻는 최적의 경로를 찾을 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제21.Grid world 환경에서 MDP 의 구성요소를 정리하면 어떻게 됩니까 ?\n",
    "![fig](http://cfile293.uf.daum.net/image/99C491495F5D6AD2151A0B)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제22.정책이란 무엇입니까 ?\n",
    "![fig](http://cfile251.uf.daum.net/image/991653395F5D6CCD16CA8E)\n",
    "$$↓$$\n",
    "![fig2](http://cfile295.uf.daum.net/image/998E833D5F5D7FC51C7359)\n",
    "\n",
    "    각 상태에서 어떻게 행동할지에 대한 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제23.정책을 수학식으로 어떻게 나타냅니까 ?\n",
    "![fig](http://cfile263.uf.daum.net/image/99E4DF345F5D80B21AECE4)\n",
    "\n",
    "    에이전트가 강화학습을 통해 학습 해야할 것은 수많은 정책 중에서 최적 정책 입니다. \n",
    "    최적 정책은 각 상태에서 단 하나의 행동만을 선택합니다. \n",
    "    하지만 에이전트가 학습하고 있을 때는 정책이 하나의 행동만을 선택하기보다는 확률적으로 여러 개의 행동을 선택할 수 있어야 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제24. 강화 학습은 현재의 정책보다 더 좋은 정책을 알아내기 위해 학습해 나가는 것입니다. 그러면 현재의 정책보다 더 좋은 정책을 찾아나가기 위해서는 에이전트가 다음 그림처럼 환경에 대해서 행동을 하고 환경은 그 행동에 대한 보상을 하면서 에이전트는 실제로 받은 보상을 토대로 자신의 정책을 바꿔 나갑니다. 그래서 결국 얻게 될 정책은 무슨 정책 입니까 ?\n",
    "![fig](http://cfile259.uf.daum.net/image/99C28C495F5D840C1E6904)\n",
    "\n",
    "    가장 많은 보상을 받게하는 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>2장. 벨만 방정식 이론 문제</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제1. 앞장의 내용을 복습 하세요.\n",
    "![fig](http://cfile260.uf.daum.net/image/99372B425F5D89241F2AAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제2. 아래와 같이 어떤 특정한 상태에 에이전트가 있다고 가정해 보겠습니다. 이 에이전트 입장에서 현재 상태에서 앞으로 받을 보상들을 고려해서 선택해야 좋은 선택을 할 수 있습니다.  하지만 아직 받지 않은 많은 보상들을 어떻게 해야 고려할 수 있을까요?  이를 위해 강화학습에서 나온 개념은 무엇입니까?\n",
    "    가치함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제3. 가치함수를 수학식으로 나타내면 무엇입니까 ?\n",
    "![fig](http://cfile288.uf.daum.net/image/991C313E5F5D92E724F7F6)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제4. 단기적인 보상만을 고려하면 안되고 장기적인 보상을 고려해야 하는 이유가 무엇인가요 ? \n",
    "    대부분의 환경에서의 보상은 장기적인 보상이기 때문\n",
    "![fig](http://cfile259.uf.daum.net/image/99361B425F5D949B24C15D)\n",
    "\n",
    "    즉각적인 보상만 고려해서 어떤 행동이 좋은 행동이었는지 판단한다면 다음과 같은 상황이 발생한다\n",
    "        마지막 행동만 좋은 행동이고 나머지는 아니라고 판단\n",
    "![fig2](http://cfile255.uf.daum.net/image/99DB64445F5D958E231F51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
