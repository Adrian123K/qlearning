{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    목요일까지 조별 포트폴리오 계획서\n",
    "        주제 : \n",
    "        역할 : \n",
    "        계획날짜 \n",
    "            1주\n",
    "            2주\n",
    "            3주\n",
    "            4주\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>[쉬움주의] 강화학습 1장 이론_문제 </b>\n",
    "#### ▩ 문제1. 강화학습의 '강화' 는 어디서 유래가 된 개념입니까? \n",
    "    '강화'라는 개념은 행동 심리학에서 유래된 개념. 동물들이 시행착오를 통해 학습하는 방법 중 하나.\n",
    "    동물들이 이전에 배우지 않았으나 직접 시도하면서 행동과 그 결과로 나타나는 좋은 보상 사이의 상관관계를 학습하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제2. 그러면 심리학에서 말하는 '시행착오' 학습이란 무엇입니까 ?\n",
    "    시행착오 학습은 동물들이 이것저것 시도해보면서 그 결과를 통해 학습하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제3. 강화학습(Reinforcement learning) 에서 '강화' 가 무슨 뜻 입니까? \n",
    "    위의 쥐 실험 예시처럼 동물이 이전에 배우지 않았지만 직접 시도하면서 행동과 그 결과로 나타나는 좋은 보상 사이의 상관관계를 학습.\n",
    "    그러면서 동물이 좋은 보상을 얻게 해주는 행동을 점점 더 많이 하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제4. 그러면 강화학습(Reinforcement learning) 은 무엇입니까 ?\n",
    "    Reinforcement + Machine Learning\n",
    "    Machine Learning은 컴퓨터가 데이터로부터 유용한 지식을 추출해 새로운 데이터에 대한 판단에 적용하는 것\n",
    "![fig](http://cfile252.uf.daum.net/image/998B46355F5AEC4A27A99D)\n",
    "![fig2](http://cfile272.uf.daum.net/image/997512495F5AED672E167C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제5. 머신러닝의 종류 3가지는 무엇입니까 ?\n",
    "    1. Supervised Learning : 정답이 있는 데이터 학습\n",
    "    2. Unsupervised Learning : 데이터 자체의 특성을 학습\n",
    "    3. Reinforcement Learning : 보상으로부터 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제6. 강화학습에서 에이전트가 무엇입니까 ?\n",
    "    대리인, 강화학습을 통해 스스로 학습하는 컴퓨터를 말한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제7. 에이전트와 환경의 상호 작용을 설명하세요.\n",
    "![fig](http://cfile277.uf.daum.net/image/9981D1495F5AEFDE2E73DF)\n",
    "\n",
    "    에이전트는 환경에 대한 사전 지식이 없는 상태에서 학습\n",
    "    자신이 놓인 환경에서 자신의 상태를 인식한 후 행동. 환경은 에이전트에게 보상을 주고 다음 상태를 알려준다.\n",
    "    이러한 보상을 지속해서 얻는닫면 에이전트는 좋은 행동을 학습할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제8. 강화학습의 장점이 무엇입니까 ?\n",
    "    환경에 대한 정확한 지식이 없어도 학습을 할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제9. 강화학습에 대해서 배울 내용들이 무엇이 있습니까 ?\n",
    "    1. 강화학습이 풀고자 하는 문제\n",
    "        Sequential Decision Problem\n",
    "    2. 문제에 대한 수학적 정의\n",
    "        Markov Decision Process\n",
    "    3. MDP를 계산으로 푸는 방법 \n",
    "        Dynamic Programming\n",
    "    4. MDP를 학습으로 푸는 방법\n",
    "        Reinforcement Learning\n",
    "    5. 상태공간이 크고 차원이 높을 때 쓰는 방법\n",
    "        Function Approximation\n",
    "    6. 바둑과 같이 복잡하고 어려운 문제를 푸는 방법\n",
    "        Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제10. 이 책도 RNN 책 처럼 어떤 문제를 해결하기 위한 접근 방식이 있습니다. 무엇인가요 ?\n",
    "    1. 해결해야하는 문제 자체에 대한 이해\n",
    "    2. 그 문제에 적용되었던 초기 방식들\n",
    "    3. 초기 방식의 문제와 해결하기 위한 아이디어\n",
    "    4. 아이디어를 구체화 하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제11.  강화학습을 공부하는 큰 기본 흐름이 어떻게 됩니까? \n",
    "    1. 순차적 결정 문제와 MDP 이해\n",
    "    2. MDP 문제를 풀기위한 Dynamic Programming\n",
    "    3. Dynamic Programming의 문제와 이를 해결하기 위한 아이디어\n",
    "        Data를 통해 학습이 되어지게 한다\n",
    "    4. 아이디어의 알고리즘화 -> 살사, QLearning\n",
    "    5. QLearning + 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제12.  강화학습은 결정을 순차적으로 내려야 하는 문제에 적용됩니다. 순차적 행동 결정 문제란 무엇인가요 ?\n",
    "    여러번의 연속적 선택을 해야하는 문제. 행동을 한 번 하는 것이 아니라 연속적(계속적)으로 선택해야하는 문제를 말한다.\n",
    "![fig](http://cfile258.uf.daum.net/image/9988AD375F5B050B3BF7A0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제12-2.  에이전트가 학습하고 발전하려면 문제를 무엇으로 표현해야합니까 ?\n",
    "    수학적으로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제13.  수학적으로 정의할 때 어떤 용어를 사용해야하는 데 그 용어의 구성요소가 무엇입니까?\n",
    "    MDP(Markov Decision Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제14. 왜 수학적으로 정의해야하나요  ?\n",
    "    수학적으로 정의해야 알고리즘을 적용했을 때 에이전트가 잘하는지 못하는지 점수로 알 수 있기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제15. MDP 의 구성요소가 무엇인가요? \n",
    "    1. 상태(State)\n",
    "    2. 행동(Action)\n",
    "    3. 보상(Reward)\n",
    "    4. 상태변환 확률(State Transition Probability)\n",
    "    5. 감가율(할인율)(Discount Factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ (점심시간 문제) grid world 환경을 구현하고 실행하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import GraphicDisplay\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import Button\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import ImageTk, Image\n",
    "\n",
    "PhotoImage = ImageTk.PhotoImage\n",
    "UNIT = 100  # 픽셀 수a\n",
    "HEIGHT = 5  # 그리드월드 세로\n",
    "WIDTH = 5  # 그리드월드 가로\n",
    "TRANSITION_PROB = 1\n",
    "POSSIBLE_ACTIONS = [0, 1, 2, 3]  # 좌, 우, 상, 하\n",
    "ACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # 좌표로 나타낸 행동\n",
    "REWARDS = []\n",
    "\n",
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.transition_probability = TRANSITION_PROB\n",
    "        self.width = WIDTH\n",
    "        self.height = HEIGHT\n",
    "        self.reward = [[0] * WIDTH for _ in range(HEIGHT)]\n",
    "        self.possible_actions = POSSIBLE_ACTIONS\n",
    "        self.reward[2][2] = 1  # (2,2) 좌표 동그라미 위치에 보상 1\n",
    "        self.reward[1][2] = -1  # (1,2) 좌표 세모 위치에 보상 -1\n",
    "        self.reward[2][1] = -1  # (2,1) 좌표 세모 위치에 보상 -1\n",
    "        self.all_state = []\n",
    "\n",
    "        for x in range(WIDTH):\n",
    "            for y in range(HEIGHT):\n",
    "                state = [x, y]\n",
    "                self.all_state.append(state)\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        next_state = self.state_after_action(state, action)\n",
    "        return self.reward[next_state[0]][next_state[1]]\n",
    "\n",
    "    def state_after_action(self, state, action_index):\n",
    "        action = ACTIONS[action_index]\n",
    "        return self.check_boundary([state[0] + action[0], state[1] + action[1]])\n",
    "\n",
    "    @staticmethod\n",
    "    def check_boundary(state):\n",
    "        state[0] = (0 if state[0] < 0 else WIDTH - 1\n",
    "                    if state[0] > WIDTH - 1 else state[0])\n",
    "        state[1] = (0 if state[1] < 0 else HEIGHT - 1\n",
    "                    if state[1] > HEIGHT - 1 else state[1])\n",
    "        return state\n",
    "\n",
    "    def get_transition_prob(self, state, action):\n",
    "        return self.transition_probability\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return self.all_state\n",
    "\n",
    "env = Env()    \n",
    "\n",
    "print(env.get_all_states())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
